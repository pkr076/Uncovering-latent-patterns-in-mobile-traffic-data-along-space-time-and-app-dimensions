{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colrs\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as spark_functions\n",
    "import pyspark.sql.types as spark_types\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType,udf,broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['CMU Serif Roman'] + plt.rcParams['font.serif']\n",
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = os.getlogin()\n",
    "WORKING_DIR = f'/home/{USER}/data/Land_use'\n",
    "DATA_DIR = f'{WORKING_DIR}/data'\n",
    "METROPOLES_SHAPE = f'{DATA_DIR}/cities'\n",
    "IMG_DIR = f'{WORKING_DIR}/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 13:46:12,594 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2023-08-07 13:46:12,940 WARN spark.SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master('spark://santiago:7077')\\\n",
    "    .appName('Land use - SRCA median week')\\\n",
    "    .config('spark.network.timeout', 300)\\\n",
    "    .config('spark.dynamicAllocation.enabled', 'true')\\\n",
    "    .config('spark.shuffle.service.enabled', 'true')\\\n",
    "    .config('spark.dynamicAllocation.initialExecutors', 1)\\\n",
    "    .config('spark.dynamicAllocation.maxExecutors', 20)\\\n",
    "    .config('spark.dynamicAllocation.minExecutors', 0)\\\n",
    "    .config('spark.driver.maxResultSize', '120g')\\\n",
    "    .config('spark.executor.cores', 1)\\\n",
    "    .config('spark.executor.memory', '4g')\\\n",
    "    .config('spark.memory.fraction', 0.6)\\\n",
    "    .config('spark.cores.max', 20)\\\n",
    "    .config('spark.executor.memoryOverhead', '8g')\\\n",
    "    .config('spark.driver.memoryOverhead', '8g')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set('spark.sql.session.timeZone', 'Europe/Paris')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CITY_NAME = 'Lyon'\n",
    "df_mask = pd.read_pickle(f'{DATA_DIR}/df_masks.pkl')\n",
    "city_row = df_mask[df_mask['name'] == CITY_NAME].iloc[0]\n",
    "\n",
    "city_shape = city_row['shape']\n",
    "city_mask = city_row['mask']\n",
    "city_left_x = city_row['left_x']\n",
    "city_bottom_y = city_row['bottom_y']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+--------+-----------+--------------------+\n",
      "|city|                app|    time|day_of_week|         traffic_map|\n",
      "+----+-------------------+--------+-----------+--------------------+\n",
      "|Lyon|Amazon Web Services|02:30:00|  Wednesday|[[0.0, 0.0, 0.0, ...|\n",
      "|Lyon|Amazon Web Services|08:00:00|  Wednesday|[[0.0, 0.0, 0.0, ...|\n",
      "+----+-------------------+--------+-----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filename = f'hdfs://santiago:9000/land_use/{CITY_NAME}_traffic_maps_median_week.parquet'\n",
    "sdf_traffic = spark.read.parquet(filename)\n",
    "#sdf_traffic = sdf_traffic.withColumnRenamed('median_week_traffic_map', 'traffic_map')\n",
    "sdf_traffic.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    'fornite',\n",
    "    'google docs',\n",
    "    'skydrive',\n",
    "    'miscrosoft store',\n",
    "    'molotov tv',\n",
    "    'orange tv',\n",
    "    'team viewer',\n",
    "    'tor',\n",
    "    'web adult'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22848"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_traffic.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some_rows = sdf_traffic.take(10)\n",
    "# data = []\n",
    "# for row in some_rows:\n",
    "#     data.append(row.asDict())\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "# sdf_small = spark.createDataFrame(df)\n",
    "# sdf_small.show(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symmetric RCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic maps and Traffic per app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_traffic_map = spark_types.ArrayType(spark_types.ArrayType(spark_types.FloatType()))\n",
    "\n",
    "@pandas_udf(schema_traffic_map)\n",
    "def total_traffic_map(traffic_maps: pd.Series)-> schema_traffic_map:\n",
    "    traffic_maps = traffic_maps.apply(lambda traffic_map: np.array(list(traffic_map)))\n",
    "    traffic_map = traffic_maps.sum(axis=0)\n",
    "\n",
    "    return traffic_map.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_sum = spark_types.FloatType()\n",
    "\n",
    "@udf(schema_sum)\n",
    "def apply_sum(traffic_map) -> schema_sum:\n",
    "    traffic_map = np.array(list(traffic_map))\n",
    "    return float(np.sum(traffic_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----------+--------------------+-------------+\n",
      "|city|    time|day_of_week|         traffic_map|      traffic|\n",
      "+----+--------+-----------+--------------------+-------------+\n",
      "|Lyon|09:00:00|  Wednesday|[[0.0, 0.0, 0.0, ...|3.79559215E11|\n",
      "|Lyon|14:30:00|   Thursday|[[0.0, 0.0, 0.0, ...|4.03103777E11|\n",
      "+----+--------+-----------+--------------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf_traffic_time = sdf_traffic.groupBy('city', 'time','day_of_week').agg(total_traffic_map('traffic_map').alias('traffic_map'))\n",
    "sdf_traffic_time = sdf_traffic_time.withColumn('traffic', apply_sum('traffic_map'))\n",
    "sdf_traffic_time.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>time</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>traffic_map</th>\n",
       "      <th>traffic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lyon</td>\n",
       "      <td>09:00:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>3.795592e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lyon</td>\n",
       "      <td>14:30:00</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>4.031038e+11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   city      time day_of_week  \\\n",
       "0  Lyon  09:00:00   Wednesday   \n",
       "1  Lyon  14:30:00    Thursday   \n",
       "\n",
       "                                         traffic_map       traffic  \n",
       "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  3.795592e+11  \n",
       "1  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  4.031038e+11  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_traffic_time = sdf_traffic_time.toPandas()\n",
    "df_traffic_time['traffic_map'] = df_traffic_time['traffic_map'].apply(lambda traffic_map: np.array(traffic_map))\n",
    "df_traffic_time.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traffic_map_time = df_traffic_time[(df_traffic_time['time'] == '10:30:00')&(df_traffic_time['day_of_week'] == 'Monday')].iloc[0]['traffic_map']\n",
    "\n",
    "# my_cmap_traffic = cm.get_cmap('Spectral_r').copy()\n",
    "# my_cmap_traffic.set_under('w', 0)\n",
    "# norm_traffic = colrs.LogNorm(vmin=1e6, vmax=5e11)\n",
    "\n",
    "# fig = plt.figure(figsize=(6, 6))\n",
    "# plt.imshow(traffic_map_time, origin='lower', cmap=my_cmap_traffic, norm=norm_traffic)\n",
    "# plt.colorbar()\n",
    "# plt.xticks([])\n",
    "# plt.yticks([])\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symetric RCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tij = traffic of app i in location j\n",
    "# Tj = traffic of all apps in location j\n",
    "# Ti = traffic of app i in all locations\n",
    "# T = traffic of all apps in all locations\n",
    "\n",
    "schema_traffic_map = spark_types.ArrayType(spark_types.ArrayType(spark_types.DoubleType()))\n",
    "\n",
    "@udf(returnType=schema_traffic_map)\n",
    "def compute_SRCA(time, day, traffic_map) -> schema_traffic_map:\n",
    "\n",
    "    traffic_map = np.array(list(traffic_map))\n",
    "    \n",
    "    Tij = traffic_map\n",
    "    Tj = df_traffic_time[(df_traffic_time['time'] == time)&(df_traffic_time['day_of_week'] == day)].iloc[0]['traffic_map']\n",
    "    Ti = traffic_map.sum()\n",
    "    T = Tj.sum()\n",
    "\n",
    "    RCA = (Tij / Tj) / (Ti / T)\n",
    "    SRCA = (RCA - 1) / (RCA + 1)\n",
    "    SRCA[ city_mask == 0 ] = 0\n",
    "    return SRCA.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+--------+-----------+--------------------+--------------------+\n",
      "|city|                app|    time|day_of_week|         traffic_map|    traffic_map_srca|\n",
      "+----+-------------------+--------+-----------+--------------------+--------------------+\n",
      "|Lyon|Amazon Web Services|02:30:00|  Wednesday|[[0.0, 0.0, 0.0, ...|[[0.0, 0.0, 0.0, ...|\n",
      "|Lyon|Amazon Web Services|08:00:00|  Wednesday|[[0.0, 0.0, 0.0, ...|[[0.0, 0.0, 0.0, ...|\n",
      "+----+-------------------+--------+-----------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf_traffic_map_rca = sdf_traffic.withColumn('traffic_map_srca', compute_SRCA('time', 'day_of_week', 'traffic_map'))\n",
    "sdf_traffic_map_rca.show(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 13:52:16,959 WARN storage.BlockManagerMasterEndpoint: No more replicas available for broadcast_13_python !\n"
     ]
    }
   ],
   "source": [
    "filename = f'hdfs://santiago:9000/land_use/{CITY_NAME}_median_week_traffic_maps_srca.parquet'\n",
    "sdf_traffic_map_rca.write.parquet(filename, mode='overwrite')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test saved dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+--------+-----------+--------------------+--------------------+\n",
      "|city|                app|    time|day_of_week|         traffic_map|    traffic_map_srca|\n",
      "+----+-------------------+--------+-----------+--------------------+--------------------+\n",
      "|Lyon|Amazon Web Services|02:30:00|  Wednesday|[[0.0, 0.0, 0.0, ...|[[0.0, 0.0, 0.0, ...|\n",
      "|Lyon|Amazon Web Services|08:00:00|  Wednesday|[[0.0, 0.0, 0.0, ...|[[0.0, 0.0, 0.0, ...|\n",
      "+----+-------------------+--------+-----------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filename = f'hdfs://santiago:9000/land_use/{CITY_NAME}_median_week_traffic_maps_srca.parquet'\n",
    "sdf_traffic = spark.read.parquet(filename)\n",
    "#sdf_traffic = sdf_traffic.withColumnRenamed('median_week_traffic_map', 'traffic_map')\n",
    "sdf_traffic.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22848"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_traffic.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
